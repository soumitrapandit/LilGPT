{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "falstaff = open(\"input.txt\").read()\n",
    "print(len(falstaff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Datasets\n",
    "chars = set(\"\".join(falstaff))\n",
    "vocab_size = len(chars)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Mapping Dicts\n",
    "itos = {i:s for i,s in enumerate(chars)}\n",
    "stoi = {s:i for i,s in itos.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62, 6, 5, 48, 44, 13, 6, 5, 48, 46, 0, 44, 49, 4, 6, 45, 44, 5, 48, 12, 5, 44, 21, 6, 43, 44, 45, 46, 0, 46, 5, 48, 44, 47, 46, 0, 44, 62, 0, 12, 63, 46, 27, 10]\n",
      "['d', 'o', 't', 'h', ' ', 'm', 'o', 't', 'h', 'e', 'r', ' ', 'k', 'n', 'o', 'w', ' ', 't', 'h', 'a', 't', ' ', 'y', 'o', 'u', ' ', 'w', 'e', 'r', 'e', 't', 'h', ' ', \"'\", 'e', 'r', ' ', 'd', 'r', 'a', 'p', 'e', 's', '?']\n"
     ]
    }
   ],
   "source": [
    "encode = lambda input: [stoi[char] for char in input]\n",
    "decode = lambda input: [itos[token] for token in input]\n",
    "\n",
    "#test:\n",
    "enc = (encode(\"doth mother know that you wereth 'er drapes?\"))\n",
    "dec = (decode(enc))\n",
    "\n",
    "print(enc)\n",
    "print(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1115394])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Datasets\n",
    "data = torch.tensor(encode(falstaff))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1003854"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test, Train splits\n",
    "train = int(0.9 * data.shape[0])\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1003854])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = data[:train]\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([111540])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data = data[train:]\n",
    "testing_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 37163,  56174, 988834,  ..., 241947, 778008, 575488])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Loader:\n",
    "# The idea is to have random samplings from the data and then creating batches from it.\n",
    "batch_size = 4\n",
    "context_len = 8\n",
    "\n",
    "# This means that the transformer is going to process four \"streams\" of tokens, each upto 8 tokens long\n",
    "\n",
    "# I need arbitary starting points for data\n",
    "rand_idx = torch.randint(0,(training_data.shape[0] - (context_len)),(training_data.shape))\n",
    "rand_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, I need to batch these\n",
    "# Actually, I can be smarter here:\n",
    "\n",
    "def data_loader(data,batch_size,context_len):\n",
    "    rand_idx = torch.randint(0,(data.shape[0] - context_len),(batch_size,))\n",
    "    X = torch.stack([data[idx:idx+context_len] for idx in rand_idx])\n",
    "    Y = torch.stack([data[idx+1:idx+context_len+1] for idx in rand_idx])\n",
    "    X = X.to(device)\n",
    "    Y = Y.to(device)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb.shape=torch.Size([4, 8]), yb.shape=torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = data_loader(training_data,batch_size=4,context_len=8)\n",
    "print(f\"{xb.shape=}, {yb.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters:\n",
    "embedding_dim = 32\n",
    "eval_iters = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the basic Bigram Model:\n",
    "\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_dim):\n",
    "        super().__init__()\n",
    "        self.K = nn.Linear(embedding_dim,head_dim,bias=False)\n",
    "        self.V = nn.Linear(embedding_dim,head_dim,bias=False)\n",
    "        self.Q = nn.Linear(embedding_dim,head_dim,bias=False)\n",
    "        self.register_buffer(\"Tril\",tensor=torch.tril(torch.ones((context_len,context_len))))\n",
    "\n",
    "    def forward(self,x):\n",
    "        B,T,C = x.shape\n",
    "        keys = self.K(x)\n",
    "        querries = self.Q(x)\n",
    "\n",
    "        #\n",
    "        wei = querries @ keys.transpose(-2,-1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.Tril[:T,:T]==0, float('inf'))\n",
    "        wei = F.softmax(wei,dim=-1)\n",
    "\n",
    "        # \n",
    "        value = self.V(x) \n",
    "        out = wei @ value # This is B * T * head_dim\n",
    "        \n",
    "        #\n",
    "        return out\n",
    "\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(num_embeddings=vocab_size,embedding_dim=embedding_dim)\n",
    "        self.position_embedding_table = nn.Embedding(context_len,embedding_dim=embedding_dim)\n",
    "        self.sa_head = Head(embedding_dim)\n",
    "        self.lm_head = nn.Linear(embedding_dim,vocab_size) # This is the last layer of the network\n",
    "\n",
    "    def forward(self, x,y=None):\n",
    "        # X and Y are both B * T at this point\n",
    "        B,T = x.shape\n",
    "\n",
    "        #Embed into character Embedding space:\n",
    "        token = self.embedding_table(x) # B * T * embedding_dim\n",
    "        pos = self.position_embedding_table(torch.arange(T,device=device)) # T * embedding_dim\n",
    "        x = token + pos # B,T,embedding_dim\n",
    "        x = self.sa_head(x)\n",
    "        logits = self.lm_head(x) # B * T * Vocab Size\n",
    "\n",
    "        loss = None\n",
    "        # Logits encode each x as a 65 dim vector\n",
    "        #print(f\"{logits.shape=}\")\n",
    "        if y is not None:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = y.view(B*T)\n",
    "\n",
    "            loss = F.cross_entropy(logits,targets)\n",
    "\n",
    "        return logits ,loss\n",
    "\n",
    "    def generate(self, input, max_tokens):\n",
    "        for _ in range(max_tokens):\n",
    "            # We get the raw scores:\n",
    "            input = input[:,-context_len:]\n",
    "            logits, loss = self(input)\n",
    "\n",
    "            #logits = B, T, C\n",
    "            #We just want the embedding for the last token\n",
    "            #So we pick the last Time (T) dimension\n",
    "\n",
    "            pred = logits[:,-1,:]\n",
    "\n",
    "            #And now we can convert these raw scores into probabilities\n",
    "\n",
    "            prob = F.softmax(pred) # converts each into probability\n",
    "\n",
    "            # And then we can sample from this distribution\n",
    "\n",
    "            output = torch.multinomial(prob,1)\n",
    "\n",
    "            input = torch.cat((input,output),dim=1)\n",
    "        \n",
    "        return input\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(nan, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Tests:\n",
    "test_model = BigramModel()\n",
    "test_model.to(device=device)\n",
    "logits, loss = test_model.forward(xb,yb)\n",
    "print(f\"{loss=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[44, 12,  0, 46, 44, 21,  6, 43],\n",
       "        [36,  2,  0,  6, 49, 46, 37, 24],\n",
       "        [ 6, 21, 12, 31, 44, 63,  0, 17],\n",
       "        [56, 38, 51, 42,  9, 24, 60, 21]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt ,yt= data_loader(training_data,batch_size=4,context_len=8)\n",
    "xtest = xt[:4].to(device=device)\n",
    "xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smtrp\\AppData\\Local\\Temp\\ipykernel_13780\\2462181954.py:76: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  prob = F.softmax(pred) # converts each into probability\n"
     ]
    }
   ],
   "source": [
    "out1 = test_model.generate(xt, max_tokens= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e youQ3zM'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fuck Yeah.\n",
    "\"\".join(decode(out1[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Time:\n",
    "\n",
    "optimizar = torch.optim.AdamW(test_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[218], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m     optimizar\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 12\u001b[0m     \u001b[43moptimizar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\smtrp\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    491\u001b[0m             )\n\u001b[1;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\smtrp\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\smtrp\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\optim\\adamw.py:215\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;129m@_use_grad_for_differentiable\u001b[39m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    209\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform a single optimization step.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m        closure (Callable, optional): A closure that reevaluates the model\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03m            and returns the loss.\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 215\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_graph_capture_health_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\smtrp\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\optim\\optimizer.py:436\u001b[0m, in \u001b[0;36mOptimizer._cuda_graph_capture_health_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_cuda_graph_capture_health_check\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;66;03m# Note [torch.compile x capturable]\u001b[39;00m\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;66;03m# If we are compiling, we try to take the capturable path automatically by\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;66;03m# Thus, when compiling, inductor will determine if cudagraphs\u001b[39;00m\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;66;03m# can be enabled based on whether there is input mutation or CPU tensors.\u001b[39;00m\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    432\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcompiler\u001b[38;5;241m.\u001b[39mis_compiling()\n\u001b[0;32m    433\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_built()\n\u001b[0;32m    434\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\n\u001b[0;32m    435\u001b[0m     ):\n\u001b[1;32m--> 436\u001b[0m         capturing \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_current_stream_capturing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m capturing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m    439\u001b[0m             group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups\n\u001b[0;32m    440\u001b[0m         ):\n\u001b[0;32m    441\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    442\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting CUDA graph capture of step() for an instance of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    443\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    444\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but param_groups\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m capturable is False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\smtrp\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\cuda\\graphs.py:30\u001b[0m, in \u001b[0;36mis_current_stream_capturing\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_current_stream_capturing\u001b[39m():\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return True if CUDA graph capture is underway on the current CUDA stream, False otherwise.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    If a CUDA context does not exist on the current device, returns False without initializing the context.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_cuda_isCurrentStreamCapturing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "steps = 1000\n",
    "for _ in range(steps):\n",
    "    # Create minibatch:\n",
    "    xb, yb = data_loader(training_data,batch_size=batch_size,context_len=context_len)\n",
    "\n",
    "    # Get Loss\n",
    "    logits,loss = test_model(xb,yb)\n",
    "\n",
    "    # Train:\n",
    "    optimizar.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizar.step()\n",
    "\n",
    "print(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smtrp\\AppData\\Local\\Temp\\ipykernel_13780\\2557073715.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  prob = F.softmax(pred) # converts each into probability\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are\n",
      "drawh wker Od, plespans I owharadd bondefreid gepat buakeveroorashy ga loty ct ice he abase e sthitono F ogimindu'd y, yath ithor a l, d his!\n",
      "RI tre ors.\n",
      "\n",
      "!\n",
      "LIC3$Gy.\n",
      "IUL:s h beey t.\n",
      "r ave atthail wFreelencep-and lics, pye LUMo nth. t bulmes asthineng!ver ast;\n",
      "F, id yom we:\n",
      "Flof ve il at, hon I ferr, lll you\n",
      "Ang m pt.\n",
      "NCHau ply,\n",
      "T lveft ber wher ta vVtmithioupy tirie y acrrild whe thastuset'hariiVMos jy Vut oulloHou ESaky t listhe ll,\n",
      "loghirenot, pthanweren-y t:\n",
      "OHES! yeoring the ; sreare, btistyt tl\n"
     ]
    }
   ],
   "source": [
    "# Test Optimization:\n",
    "out2 = test_model.generate(xt,max_tokens=500)\n",
    "print(\"\".join(decode(out2[0].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Self Attention </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Some Experiments: \n",
    "B,T,C = 4, 8, 2\n",
    "x = torch.randn((B,T,C))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Tril\n",
    "tril = torch.tril(torch.ones((B,T,C)))\n",
    "tril.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So I want tril to only work on the Time Dimension\n",
    "# How do I do that?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
